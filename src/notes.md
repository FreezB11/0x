-> tokenization
<!-- breaking the words into tokens -->
<!-- then converting the tokens to vectors -->
-> encoding
<!-- this what actually converts the tokens to vectors -->
-> attention block 
<!-- smtg like the vectors talk to eachother -->
<!-- what happens is basically the attention block updates the values of the vector based of the previous values -->
-> MLP(multi layer perceptron)

// now all this proces happens again and again